{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ee22df-4f7e-486c-9acf-3e98f400d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FACE DETECTION PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696a0b6a-36d7-4294-b820-2eb94a8656c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AlreadyExistsError",
     "evalue": "Another metric with the same name already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Prevent duplicate registrations by checking if GPUs are already configured\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_logical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\tensorflow\\__init__.py:469\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_current_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    468\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 469\u001b[0m     \u001b[43m_keras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\engine\\functional.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer_utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_layer \u001b[38;5;28;01mas\u001b[39;00m input_layer_module\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast_variable\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss_scale_optimizer\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m policy\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layer_serialization\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\mixed_precision\\loss_scale_optimizer.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss_scale \u001b[38;5;28;01mas\u001b[39;00m keras_loss_scale_module\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v2\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m optimizer_utils\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:36\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_utils\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m---> 36\u001b[0m keras_optimizers_gauge \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonitoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolGauge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tensorflow/api/oss-keras/optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras optimizer usage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m _DEFAULT_VALID_DTYPES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[0;32m     40\u001b[0m     tf\u001b[38;5;241m.\u001b[39mfloat16, tf\u001b[38;5;241m.\u001b[39mbfloat16, tf\u001b[38;5;241m.\u001b[39mfloat32, tf\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m     41\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcomplex64, tf\u001b[38;5;241m.\u001b[39mcomplex128\n\u001b[0;32m     42\u001b[0m ])\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deduplicate_indexed_slices\u001b[39m(values, indices):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py:356\u001b[0m, in \u001b[0;36mBoolGauge.__init__\u001b[1;34m(self, name, description, *labels)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, description, \u001b[38;5;241m*\u001b[39mlabels):\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new BoolGauge.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    *labels: The label list of the new metric.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBoolGauge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBoolGauge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_bool_gauge_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py:131\u001b[0m, in \u001b[0;36mMetric.__init__\u001b[1;34m(self, metric_name, metric_methods, label_length, *args)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods):\n\u001b[0;32m    128\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot create \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m metric with label >= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    129\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_name, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods)))\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_methods\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_label_length\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAlreadyExistsError\u001b[0m: Another metric with the same name already exists."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Prevent duplicate registrations by checking if GPUs are already configured\n",
    "if not tf.config.list_logical_devices('GPU'):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.set_logical_device_configuration(\n",
    "                    gpu,\n",
    "                    [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])  # example: set memory limit to 4GB\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"No GPU found\")\n",
    "else:\n",
    "    print(\"GPUs are already configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315213ef-fe4b-4eb6-a3a4-bcec5003710e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AlreadyExistsError",
     "evalue": "Another metric with the same name already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure_gpus\u001b[39m(memory_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(configure_gpus, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialized\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\tensorflow\\__init__.py:469\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_current_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    468\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 469\u001b[0m     \u001b[43m_keras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\engine\\functional.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer_utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_layer \u001b[38;5;28;01mas\u001b[39;00m input_layer_module\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast_variable\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss_scale_optimizer\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m policy\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layer_serialization\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\mixed_precision\\loss_scale_optimizer.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss_scale \u001b[38;5;28;01mas\u001b[39;00m keras_loss_scale_module\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v2\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m optimizer_utils\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:36\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_utils\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m---> 36\u001b[0m keras_optimizers_gauge \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonitoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolGauge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tensorflow/api/oss-keras/optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras optimizer usage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m _DEFAULT_VALID_DTYPES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[0;32m     40\u001b[0m     tf\u001b[38;5;241m.\u001b[39mfloat16, tf\u001b[38;5;241m.\u001b[39mbfloat16, tf\u001b[38;5;241m.\u001b[39mfloat32, tf\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m     41\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcomplex64, tf\u001b[38;5;241m.\u001b[39mcomplex128\n\u001b[0;32m     42\u001b[0m ])\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deduplicate_indexed_slices\u001b[39m(values, indices):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py:356\u001b[0m, in \u001b[0;36mBoolGauge.__init__\u001b[1;34m(self, name, description, *labels)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, description, \u001b[38;5;241m*\u001b[39mlabels):\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new BoolGauge.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    *labels: The label list of the new metric.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBoolGauge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBoolGauge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_bool_gauge_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\free_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py:131\u001b[0m, in \u001b[0;36mMetric.__init__\u001b[1;34m(self, metric_name, metric_methods, label_length, *args)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods):\n\u001b[0;32m    128\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot create \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m metric with label >= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    129\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_name, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods)))\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_methods\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_label_length\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAlreadyExistsError\u001b[0m: Another metric with the same name already exists."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def configure_gpus(memory_limit=4096):\n",
    "    if not hasattr(configure_gpus, 'initialized'):\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                for gpu in gpus:\n",
    "                    tf.config.set_logical_device_configuration(\n",
    "                        gpu,\n",
    "                        [tf.config.LogicalDeviceConfiguration(memory_limit=memory_limit)])  # set memory limit\n",
    "                logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "                configure_gpus.initialized = True\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"No GPU found\")\n",
    "    else:\n",
    "        print(\"GPUs are already configured\")\n",
    "\n",
    "# Call the function to configure GPUs\n",
    "configure_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0861cbc8-ab7a-466c-9c09-f223e2732f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the new person:  cs211048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 323.6ms\n",
      "Speed: 0.0ms preprocess, 323.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img1.jpg\n",
      "\n",
      "0: 480x640 1 face, 137.7ms\n",
      "Speed: 3.1ms preprocess, 137.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img2.jpg\n",
      "\n",
      "0: 480x640 1 face, 145.4ms\n",
      "Speed: 3.7ms preprocess, 145.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img3.jpg\n",
      "\n",
      "0: 480x640 1 face, 160.3ms\n",
      "Speed: 7.5ms preprocess, 160.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img4.jpg\n",
      "\n",
      "0: 480x640 1 face, 138.9ms\n",
      "Speed: 7.9ms preprocess, 138.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img5.jpg\n",
      "\n",
      "0: 480x640 1 face, 137.3ms\n",
      "Speed: 0.0ms preprocess, 137.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img6.jpg\n",
      "\n",
      "0: 480x640 1 face, 111.1ms\n",
      "Speed: 2.8ms preprocess, 111.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img7.jpg\n",
      "\n",
      "0: 480x640 1 face, 133.2ms\n",
      "Speed: 0.5ms preprocess, 133.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img8.jpg\n",
      "\n",
      "0: 480x640 1 face, 132.2ms\n",
      "Speed: 3.8ms preprocess, 132.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img9.jpg\n",
      "\n",
      "0: 480x640 1 face, 148.0ms\n",
      "Speed: 3.5ms preprocess, 148.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img10.jpg\n",
      "\n",
      "0: 480x640 1 face, 160.7ms\n",
      "Speed: 2.0ms preprocess, 160.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img11.jpg\n",
      "\n",
      "0: 480x640 1 face, 131.2ms\n",
      "Speed: 0.0ms preprocess, 131.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img12.jpg\n",
      "\n",
      "0: 480x640 1 face, 175.8ms\n",
      "Speed: 0.0ms preprocess, 175.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img13.jpg\n",
      "\n",
      "0: 480x640 1 face, 120.4ms\n",
      "Speed: 6.4ms preprocess, 120.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img14.jpg\n",
      "\n",
      "0: 480x640 1 face, 144.9ms\n",
      "Speed: 7.0ms preprocess, 144.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img15.jpg\n",
      "\n",
      "0: 480x640 1 face, 166.2ms\n",
      "Speed: 7.4ms preprocess, 166.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img16.jpg\n",
      "\n",
      "0: 480x640 1 face, 152.0ms\n",
      "Speed: 0.0ms preprocess, 152.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img17.jpg\n",
      "\n",
      "0: 480x640 1 face, 145.1ms\n",
      "Speed: 0.0ms preprocess, 145.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img18.jpg\n",
      "\n",
      "0: 480x640 1 face, 144.7ms\n",
      "Speed: 0.0ms preprocess, 144.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img19.jpg\n",
      "\n",
      "0: 480x640 1 face, 127.7ms\n",
      "Speed: 0.0ms preprocess, 127.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Captured known_faces\\cs211048\\cs211048_img20.jpg\n"
     ]
    }
   ],
   "source": [
    "## TAKING IMAGES WALA PART\n",
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "if not os.path.exists(known_faces_dir):\n",
    "    os.makedirs(known_faces_dir)\n",
    "\n",
    "def capture_images(name):\n",
    "    # Create a directory for the new person\n",
    "    person_dir = os.path.join(known_faces_dir, name)\n",
    "    if not os.path.exists(person_dir):\n",
    "        os.makedirs(person_dir)\n",
    "    \n",
    "    # Open a connection to the webcam\n",
    "    cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    images_captured = 0\n",
    "    while images_captured < 20 and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "        \n",
    "        # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Perform inference\n",
    "        results = model(rgb_frame)\n",
    "        \n",
    "        # Convert results to a DataFrame-like structure\n",
    "        boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "        for box in boxes:\n",
    "            # Extract the coordinates and confidence score\n",
    "            x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "            \n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # Capture the face ROI\n",
    "            face_roi = frame[y1:y2, x1:x2]\n",
    "            rgb_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Save the image\n",
    "            #\"farree.\"+str(img_id)+'.jpg'\n",
    "            file_path = os.path.join(person_dir, f'{name}_img{images_captured + 1}.jpg')\n",
    "            cv2.imwrite(file_path, rgb_face)\n",
    "            print(f\"Captured {file_path}\")\n",
    "            images_captured += 1\n",
    "            \n",
    "            # Display the captured image\n",
    "            cv2.imshow('Captured Face', rgb_face)\n",
    "            cv2.waitKey(100)  # Pause to display the image\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Face Capture', frame)\n",
    "        \n",
    "        # Exit on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and destroy all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "name = input(\"Enter the name of the new person: \")\n",
    "capture_images(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b981e8-6d9f-42ae-bb4a-152832703542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognizing part\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if name not in known_face_names:\n",
    "    print(f\"Error: {name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e714c286-2f70-4c71-9647-fe5a63ece03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 459.4ms\n",
      "Speed: 16.3ms preprocess, 459.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 181.8ms\n",
      "Speed: 2.2ms preprocess, 181.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 137.3ms\n",
      "Speed: 0.0ms preprocess, 137.3ms inference, 9.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 142.2ms\n",
      "Speed: 1.8ms preprocess, 142.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 146.3ms\n",
      "Speed: 1.3ms preprocess, 146.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 120.4ms\n",
      "Speed: 0.0ms preprocess, 120.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 109.7ms\n",
      "Speed: 0.0ms preprocess, 109.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 129.1ms\n",
      "Speed: 0.0ms preprocess, 129.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 126.7ms\n",
      "Speed: 0.0ms preprocess, 126.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 118.4ms\n",
      "Speed: 0.0ms preprocess, 118.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 107.1ms\n",
      "Speed: 3.9ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 108.6ms\n",
      "Speed: 0.0ms preprocess, 108.6ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 104.2ms\n",
      "Speed: 0.0ms preprocess, 104.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 104.5ms\n",
      "Speed: 2.1ms preprocess, 104.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 107.3ms\n",
      "Speed: 0.0ms preprocess, 107.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 127.2ms\n",
      "Speed: 2.1ms preprocess, 127.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 135.1ms\n",
      "Speed: 0.0ms preprocess, 135.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 123.0ms\n",
      "Speed: 0.0ms preprocess, 123.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 104.6ms\n",
      "Speed: 2.3ms preprocess, 104.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 118.6ms\n",
      "Speed: 0.0ms preprocess, 118.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 118.0ms\n",
      "Speed: 2.3ms preprocess, 118.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 142.5ms\n",
      "Speed: 0.0ms preprocess, 142.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 121.8ms\n",
      "Speed: 0.0ms preprocess, 121.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 111.4ms\n",
      "Speed: 0.0ms preprocess, 111.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 120.2ms\n",
      "Speed: 0.5ms preprocess, 120.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 133.0ms\n",
      "Speed: 1.7ms preprocess, 133.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 120.0ms\n",
      "Speed: 2.5ms preprocess, 120.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 113.0ms\n",
      "Speed: 4.0ms preprocess, 113.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 117.3ms\n",
      "Speed: 0.0ms preprocess, 117.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 110.9ms\n",
      "Speed: 0.0ms preprocess, 110.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 133.9ms\n",
      "Speed: 1.9ms preprocess, 133.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 111.4ms\n",
      "Speed: 0.0ms preprocess, 111.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.8ms\n",
      "Speed: 0.0ms preprocess, 124.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 107.0ms\n",
      "Speed: 1.9ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 143.8ms\n",
      "Speed: 0.0ms preprocess, 143.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 148.9ms\n",
      "Speed: 2.8ms preprocess, 148.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 108.7ms\n",
      "Speed: 0.0ms preprocess, 108.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 126.2ms\n",
      "Speed: 0.0ms preprocess, 126.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 120.8ms\n",
      "Speed: 0.0ms preprocess, 120.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 107.3ms\n",
      "Speed: 3.9ms preprocess, 107.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 118.3ms\n",
      "Speed: 0.0ms preprocess, 118.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 126.3ms\n",
      "Speed: 0.0ms preprocess, 126.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 128.9ms\n",
      "Speed: 3.2ms preprocess, 128.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 106.0ms\n",
      "Speed: 0.0ms preprocess, 106.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 111.2ms\n",
      "Speed: 0.0ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 117.7ms\n",
      "Speed: 0.0ms preprocess, 117.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 103.6ms\n",
      "Speed: 0.0ms preprocess, 103.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 129.0ms\n",
      "Speed: 0.0ms preprocess, 129.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 135.9ms\n",
      "Speed: 0.0ms preprocess, 135.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 116.7ms\n",
      "Speed: 0.0ms preprocess, 116.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 104.3ms\n",
      "Speed: 0.0ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 107.9ms\n",
      "Speed: 0.0ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 119.6ms\n",
      "Speed: 0.0ms preprocess, 119.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 127.5ms\n",
      "Speed: 0.0ms preprocess, 127.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 128.8ms\n",
      "Speed: 0.0ms preprocess, 128.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 108.6ms\n",
      "Speed: 0.0ms preprocess, 108.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 135.4ms\n",
      "Speed: 0.0ms preprocess, 135.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 127.6ms\n",
      "Speed: 0.0ms preprocess, 127.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 108.2ms\n",
      "Speed: 0.0ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 113.8ms\n",
      "Speed: 0.0ms preprocess, 113.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 138.3ms\n",
      "Speed: 0.0ms preprocess, 138.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 119.2ms\n",
      "Speed: 0.0ms preprocess, 119.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 103.2ms\n",
      "Speed: 1.8ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 116.7ms\n",
      "Speed: 2.0ms preprocess, 116.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 119.8ms\n",
      "Speed: 1.8ms preprocess, 119.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 111.8ms\n",
      "Speed: 0.0ms preprocess, 111.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 112.9ms\n",
      "Speed: 1.8ms preprocess, 112.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 102.5ms\n",
      "Speed: 1.7ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 106.8ms\n",
      "Speed: 0.0ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "## RECOGNIZING FACE WITHOUT TAKING INPUT\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411d1b6-12bb-4cb4-8d8b-798503da4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "input_name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if input_name not in known_face_names:\n",
    "    print(f\"Error: {input_name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    recognized_name = known_face_names[best_match_index]\n",
    "                    if recognized_name == input_name:\n",
    "                        name = recognized_name\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07fa165-f8db-4005-8748-4be515d41d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  fareesa siddiqui\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 309.3ms\n",
      "Speed: 11.2ms preprocess, 309.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 162.0ms\n",
      "Speed: 1.6ms preprocess, 162.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 162.0ms\n",
      "Speed: 0.6ms preprocess, 162.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 118.7ms\n",
      "Speed: 0.0ms preprocess, 118.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 129.0ms\n",
      "Speed: 0.0ms preprocess, 129.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 175.5ms\n",
      "Speed: 0.0ms preprocess, 175.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 134.7ms\n",
      "Speed: 3.0ms preprocess, 134.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 117.2ms\n",
      "Speed: 2.3ms preprocess, 117.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.8ms\n",
      "Speed: 2.6ms preprocess, 124.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 133.7ms\n",
      "Speed: 2.8ms preprocess, 133.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.7ms\n",
      "Speed: 0.0ms preprocess, 124.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 127.5ms\n",
      "Speed: 2.7ms preprocess, 127.5ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 134.9ms\n",
      "Speed: 0.0ms preprocess, 134.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 114.0ms\n",
      "Speed: 0.0ms preprocess, 114.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 117.9ms\n",
      "Speed: 2.7ms preprocess, 117.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 131.5ms\n",
      "Speed: 0.0ms preprocess, 131.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 135.8ms\n",
      "Speed: 0.0ms preprocess, 135.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 113.4ms\n",
      "Speed: 0.0ms preprocess, 113.4ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 119.8ms\n",
      "Speed: 0.0ms preprocess, 119.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 139.6ms\n",
      "Speed: 0.0ms preprocess, 139.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 133.8ms\n",
      "Speed: 0.7ms preprocess, 133.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 133.4ms\n",
      "Speed: 0.0ms preprocess, 133.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 126.6ms\n",
      "Speed: 0.0ms preprocess, 126.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 146.7ms\n",
      "Speed: 0.0ms preprocess, 146.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.3ms\n",
      "Speed: 0.0ms preprocess, 124.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.9ms\n",
      "Speed: 3.2ms preprocess, 124.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 137.1ms\n",
      "Speed: 0.0ms preprocess, 137.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "##RECOGNITION PART\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if name not in known_face_names:\n",
    "    print(f\"Error: {name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df56fe-f5d5-4db8-b4d3-c6b9137fd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "## taking multiple time name in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce5b1c4-5023-4352-b691-f0470f6e7c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  fareesa siddiqui\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 334.7ms\n",
      "Speed: 12.4ms preprocess, 334.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 148.7ms\n",
      "Speed: 2.3ms preprocess, 148.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 127.1ms\n",
      "Speed: 1.0ms preprocess, 127.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.6ms\n",
      "Speed: 0.0ms preprocess, 124.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 128.9ms\n",
      "Speed: 3.5ms preprocess, 128.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 119.4ms\n",
      "Speed: 0.0ms preprocess, 119.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 143.4ms\n",
      "Speed: 2.5ms preprocess, 143.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.8ms\n",
      "Speed: 0.0ms preprocess, 124.8ms inference, 12.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 134.3ms\n",
      "Speed: 3.5ms preprocess, 134.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 115.7ms\n",
      "Speed: 0.0ms preprocess, 115.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 125.6ms\n",
      "Speed: 0.0ms preprocess, 125.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 112.0ms\n",
      "Speed: 0.0ms preprocess, 112.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 107.5ms\n",
      "Speed: 0.0ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 109.7ms\n",
      "Speed: 0.0ms preprocess, 109.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 114.9ms\n",
      "Speed: 0.0ms preprocess, 114.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 112.4ms\n",
      "Speed: 2.0ms preprocess, 112.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 142.2ms\n",
      "Speed: 0.0ms preprocess, 142.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 105.9ms\n",
      "Speed: 0.0ms preprocess, 105.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 116.6ms\n",
      "Speed: 0.0ms preprocess, 116.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Path to the attendance CSV file\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Function to append attendance to a CSV file\n",
    "def mark_attendance(name):\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        # Create the file and write the header if it doesn't exist\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "    \n",
    "    # Append the name with the current date and time\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%Y-%m-%d')\n",
    "    time_str = now.strftime('%H:%M:%S')\n",
    "    df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "    df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if name not in known_face_names:\n",
    "    print(f\"Error: {name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    mark_attendance(name)  # Mark attendance for recognized face\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784b7f4c-af04-4267-ba39-230ba0b4ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 230.4ms\n",
      "Speed: 7.5ms preprocess, 230.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 137.4ms\n",
      "Speed: 4.1ms preprocess, 137.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 128.4ms\n",
      "Speed: 4.2ms preprocess, 128.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 126.2ms\n",
      "Speed: 2.0ms preprocess, 126.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 138.7ms\n",
      "Speed: 0.0ms preprocess, 138.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 109.2ms\n",
      "Speed: 3.6ms preprocess, 109.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 113.4ms\n",
      "Speed: 0.0ms preprocess, 113.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 113.8ms\n",
      "Speed: 0.0ms preprocess, 113.8ms inference, 15.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 117.5ms\n",
      "Speed: 0.8ms preprocess, 117.5ms inference, 15.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 127.6ms\n",
      "Speed: 0.0ms preprocess, 127.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 131.4ms\n",
      "Speed: 0.0ms preprocess, 131.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 111.5ms\n",
      "Speed: 3.7ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 133.8ms\n",
      "Speed: 0.0ms preprocess, 133.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 183.9ms\n",
      "Speed: 0.0ms preprocess, 183.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Path to the attendance CSV file\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Function to append attendance to a CSV file and give voice feedback\n",
    "def mark_attendance(name):\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        # Create the file and write the header if it doesn't exist\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "    \n",
    "    # Append the name with the current date and time\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%Y-%m-%d')\n",
    "    time_str = now.strftime('%H:%M:%S')\n",
    "    df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "    df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    # Voice feedback\n",
    "    engine.say(\"Attendance is marked\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Set to keep track of names that have been recorded during the session\n",
    "recorded_names = set()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)  # Mark attendance for recognized face\n",
    "                        recorded_names.add(name)\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "392e070b-5e2c-4434-b7c5-f586c8052a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  cs211048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 452.9ms\n",
      "Speed: 5.8ms preprocess, 452.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 154.4ms\n",
      "Speed: 5.1ms preprocess, 154.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 160.3ms\n",
      "Speed: 2.0ms preprocess, 160.3ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 156.3ms\n",
      "Speed: 0.0ms preprocess, 156.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 151.1ms\n",
      "Speed: 0.0ms preprocess, 151.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 110.3ms\n",
      "Speed: 0.0ms preprocess, 110.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 141.6ms\n",
      "Speed: 3.3ms preprocess, 141.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 132.6ms\n",
      "Speed: 0.0ms preprocess, 132.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 127.8ms\n",
      "Speed: 0.0ms preprocess, 127.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 111.2ms\n",
      "Speed: 0.0ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 133.2ms\n",
      "Speed: 4.3ms preprocess, 133.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 118.9ms\n",
      "Speed: 0.0ms preprocess, 118.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 142.2ms\n",
      "Speed: 0.0ms preprocess, 142.2ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 112.5ms\n",
      "Speed: 0.0ms preprocess, 112.5ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 164.5ms\n",
      "Speed: 2.0ms preprocess, 164.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 112.0ms\n",
      "Speed: 0.0ms preprocess, 112.0ms inference, 16.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 125.9ms\n",
      "Speed: 0.0ms preprocess, 125.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 129.1ms\n",
      "Speed: 0.0ms preprocess, 129.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.5ms\n",
      "Speed: 2.7ms preprocess, 124.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 143.0ms\n",
      "Speed: 0.0ms preprocess, 143.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 116.0ms\n",
      "Speed: 0.0ms preprocess, 116.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 121.2ms\n",
      "Speed: 0.0ms preprocess, 121.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 114.7ms\n",
      "Speed: 0.0ms preprocess, 114.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.3ms\n",
      "Speed: 0.0ms preprocess, 124.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 136.7ms\n",
      "Speed: 0.0ms preprocess, 136.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 124.2ms\n",
      "Speed: 3.0ms preprocess, 124.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 140.0ms\n",
      "Speed: 0.0ms preprocess, 140.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 141.7ms\n",
      "Speed: 0.0ms preprocess, 141.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 120.0ms\n",
      "Speed: 2.7ms preprocess, 120.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 147.4ms\n",
      "Speed: 0.8ms preprocess, 147.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 167.8ms\n",
      "Speed: 0.0ms preprocess, 167.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 138.4ms\n",
      "Speed: 0.0ms preprocess, 138.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 134.5ms\n",
      "Speed: 0.0ms preprocess, 134.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 126.7ms\n",
      "Speed: 0.0ms preprocess, 126.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 134.8ms\n",
      "Speed: 2.1ms preprocess, 134.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 116.8ms\n",
      "Speed: 2.0ms preprocess, 116.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 132.6ms\n",
      "Speed: 0.0ms preprocess, 132.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 132.7ms\n",
      "Speed: 2.2ms preprocess, 132.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 165.0ms\n",
      "Speed: 2.1ms preprocess, 165.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 145.0ms\n",
      "Speed: 1.8ms preprocess, 145.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 148.3ms\n",
      "Speed: 2.0ms preprocess, 148.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 123.4ms\n",
      "Speed: 2.5ms preprocess, 123.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 114.9ms\n",
      "Speed: 0.0ms preprocess, 114.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 111.9ms\n",
      "Speed: 0.0ms preprocess, 111.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "#male voice attendence \n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Path to the attendance CSV file\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Function to append attendance to a CSV file and give voice feedback\n",
    "def mark_attendance(name):\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        # Create the file and write the header if it doesn't exist\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "    \n",
    "    # Append the name with the current date and time\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%Y-%m-%d')\n",
    "    time_str = now.strftime('%H:%M:%S')\n",
    "    df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "    df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    # Voice feedback\n",
    "    engine.say(f\"{name}, attendance is marked\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "input_name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if input_name not in known_face_names:\n",
    "    print(f\"Error: {input_name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Set to keep track of names that have been recorded during the session\n",
    "recorded_names = set()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)  # Mark attendance for recognized face\n",
    "                        recorded_names.add(name)\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b68214bf-402e-4d2a-aab0-fb7aa46b70d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  cs211048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 344.0ms\n",
      "Speed: 15.1ms preprocess, 344.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 147.1ms\n",
      "Speed: 3.3ms preprocess, 147.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 137.4ms\n",
      "Speed: 0.0ms preprocess, 137.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 126.1ms\n",
      "Speed: 2.9ms preprocess, 126.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 125.7ms\n",
      "Speed: 3.1ms preprocess, 125.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 119.5ms\n",
      "Speed: 5.3ms preprocess, 119.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 140.6ms\n",
      "Speed: 4.0ms preprocess, 140.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 129.4ms\n",
      "Speed: 4.6ms preprocess, 129.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 143.9ms\n",
      "Speed: 0.0ms preprocess, 143.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 127.7ms\n",
      "Speed: 0.0ms preprocess, 127.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 121.7ms\n",
      "Speed: 0.0ms preprocess, 121.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 153.3ms\n",
      "Speed: 2.1ms preprocess, 153.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 147.5ms\n",
      "Speed: 0.0ms preprocess, 147.5ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 153.9ms\n",
      "Speed: 0.0ms preprocess, 153.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 130.9ms\n",
      "Speed: 2.6ms preprocess, 130.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Path to the attendance CSV file\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set voice to female\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    if \"female\" in voice.name.lower():\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Function to append attendance to a CSV file and give voice feedback\n",
    "def mark_attendance(name):\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        # Create the file and write the header if it doesn't exist\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "    \n",
    "    # Append the name with the current date and time\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%Y-%m-%d')\n",
    "    time_str = now.strftime('%H:%M:%S')\n",
    "    df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "    df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    # Voice feedback\n",
    "    engine.say(f\"{name}, attendance is marked\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "input_name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if input_name not in known_face_names:\n",
    "    print(f\"Error: {input_name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Set to keep track of names that have been recorded during the session\n",
    "recorded_names = set()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)  # Mark attendance for recognized face\n",
    "                        recorded_names.add(name)\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a165652a-d0b4-47fa-ae22-4ed1690d65ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1182745644.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[28], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\n",
    "Name: Microsoft Zira Desktop - English (United States)\n",
    "Gender: female\n",
    "Language: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ac74e0c-1f35-4d3a-8822-f9b4a737bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0\n",
      "Name: Microsoft David Desktop - English (United States)\n",
      "Gender: None\n",
      "Language: []\n",
      "\n",
      "ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\n",
      "Name: Microsoft Zira Desktop - English (United States)\n",
      "Gender: None\n",
      "Language: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "for voice in voices:\n",
    "    print(f\"ID: {voice.id}\\nName: {voice.name}\\nGender: {voice.gender}\\nLanguage: {voice.languages}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f777f88b-6ab5-4620-94bc-a2594970c0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  owl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: owl is not a recognized user.\n",
      "\n",
      "0: 480x640 1 face, 577.0ms\n",
      "Speed: 6.1ms preprocess, 577.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 158.1ms\n",
      "Speed: 0.0ms preprocess, 158.1ms inference, 7.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 152.7ms\n",
      "Speed: 0.0ms preprocess, 152.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 149.3ms\n",
      "Speed: 0.0ms preprocess, 149.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 135.6ms\n",
      "Speed: 3.0ms preprocess, 135.6ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 138.1ms\n",
      "Speed: 3.3ms preprocess, 138.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 161.6ms\n",
      "Speed: 2.3ms preprocess, 161.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 166.8ms\n",
      "Speed: 1.6ms preprocess, 166.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 159.1ms\n",
      "Speed: 0.0ms preprocess, 159.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Path to the attendance CSV file\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set voice to Microsoft Zira Desktop - English (United States)\n",
    "voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "engine.setProperty('voice', voice_id)\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Function to append attendance to a CSV file and give voice feedback\n",
    "def mark_attendance(name):\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        # Create the file and write the header if it doesn't exist\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "    \n",
    "    # Append the name with the current date and time\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%Y-%m-%d')\n",
    "    time_str = now.strftime('%H:%M:%S')\n",
    "    df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "    df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    # Voice feedback\n",
    "    engine.say(f\"{name}, IDs attendance is marked\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "input_name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if input_name not in known_face_names:\n",
    "    print(f\"Error: {input_name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Set to keep track of names that have been recorded during the session\n",
    "recorded_names = set()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)  # Mark attendance for recognized face\n",
    "                        recorded_names.add(name)\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2e78d0-f4ee-4646-afe6-b2ba73e350a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  cs211048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: cs211048 is not a recognized user.\n",
      "\n",
      "0: 480x640 1 face, 353.8ms\n",
      "Speed: 9.5ms preprocess, 353.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 308.4ms\n",
      "Speed: 5.7ms preprocess, 308.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 188.7ms\n",
      "Speed: 3.4ms preprocess, 188.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 123.4ms\n",
      "Speed: 0.0ms preprocess, 123.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 140.1ms\n",
      "Speed: 0.0ms preprocess, 140.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 119.6ms\n",
      "Speed: 3.6ms preprocess, 119.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 155.0ms\n",
      "Speed: 0.0ms preprocess, 155.0ms inference, 6.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 130.0ms\n",
      "Speed: 2.0ms preprocess, 130.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 156.7ms\n",
      "Speed: 0.0ms preprocess, 156.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 146.2ms\n",
      "Speed: 1.0ms preprocess, 146.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 163.3ms\n",
      "Speed: 0.0ms preprocess, 163.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 170.3ms\n",
      "Speed: 3.0ms preprocess, 170.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 122.2ms\n",
      "Speed: 0.0ms preprocess, 122.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 146.7ms\n",
      "Speed: 0.0ms preprocess, 146.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 129.7ms\n",
      "Speed: 1.0ms preprocess, 129.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 141.4ms\n",
      "Speed: 0.0ms preprocess, 141.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 130.4ms\n",
      "Speed: 2.9ms preprocess, 130.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 136.4ms\n",
      "Speed: 0.8ms preprocess, 136.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 131.6ms\n",
      "Speed: 5.0ms preprocess, 131.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 128.8ms\n",
      "Speed: 2.7ms preprocess, 128.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Path to the attendance CSV file\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set voice to Microsoft Zira Desktop - English (United States)\n",
    "voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "engine.setProperty('voice', voice_id)\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Function to append attendance to a CSV file and give voice feedback\n",
    "def mark_attendance(name):\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        # Create the file and write the header if it doesn't exist\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "    \n",
    "    if name == \"Unknown\":\n",
    "        print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "        engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "    else:\n",
    "        # Append the name with the current date and time\n",
    "        now = datetime.now()\n",
    "        date_str = now.strftime('%Y-%m-%d')\n",
    "        time_str = now.strftime('%H:%M:%S')\n",
    "        df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "        \n",
    "        # Voice feedback\n",
    "        engine.say(f\"{name}, attendance is marked\")\n",
    "\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "input_name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if input_name not in known_face_names:\n",
    "    print(f\"Error: {input_name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Set to keep track of names that have been recorded during the session\n",
    "recorded_names = set()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)  # Mark attendance for recognized face\n",
    "                        recorded_names.add(name)\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9e8d1-988f-4a73-8bb8-2f9e386158e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  cs211048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: cs211048 is not a recognized user.\n",
      "\n",
      "0: 480x640 1 face, 381.4ms\n",
      "Speed: 14.7ms preprocess, 381.4ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 182.0ms\n",
      "Speed: 5.6ms preprocess, 182.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 133.5ms\n",
      "Speed: 0.0ms preprocess, 133.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 142.3ms\n",
      "Speed: 2.7ms preprocess, 142.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 143.3ms\n",
      "Speed: 0.0ms preprocess, 143.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 122.0ms\n",
      "Speed: 2.5ms preprocess, 122.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 143.7ms\n",
      "Speed: 3.9ms preprocess, 143.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 138.5ms\n",
      "Speed: 0.0ms preprocess, 138.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 143.6ms\n",
      "Speed: 3.1ms preprocess, 143.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 144.7ms\n",
      "Speed: 2.9ms preprocess, 144.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 185.8ms\n",
      "Speed: 2.3ms preprocess, 185.8ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 166.1ms\n",
      "Speed: 5.8ms preprocess, 166.1ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 171.2ms\n",
      "Speed: 2.4ms preprocess, 171.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 126.4ms\n",
      "Speed: 5.1ms preprocess, 126.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 146.3ms\n",
      "Speed: 3.7ms preprocess, 146.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 148.4ms\n",
      "Speed: 0.0ms preprocess, 148.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 154.0ms\n",
      "Speed: 0.0ms preprocess, 154.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 157.2ms\n",
      "Speed: 2.5ms preprocess, 157.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 143.8ms\n",
      "Speed: 4.5ms preprocess, 143.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 148.9ms\n",
      "Speed: 5.7ms preprocess, 148.9ms inference, 7.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 169.0ms\n",
      "Speed: 2.3ms preprocess, 169.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 136.3ms\n",
      "Speed: 0.0ms preprocess, 136.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Path to the attendance CSV file\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set voice to Microsoft Zira Desktop - English (United States)\n",
    "voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "engine.setProperty('voice', voice_id)\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Function to append attendance to a CSV file and give voice feedback\n",
    "def mark_attendance(name):\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        # Create the file and write the header if it doesn't exist\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "    \n",
    "    if name == \"Unknown\":\n",
    "        print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "        engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "    else:\n",
    "        # Append the name with the current date and time\n",
    "        now = datetime.now()\n",
    "        date_str = now.strftime('%Y-%m-%d')\n",
    "        time_str = now.strftime('%H:%M:%S')\n",
    "        df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "        \n",
    "        # Voice feedback\n",
    "        engine.say(f\"{name}, attendance is marked\")\n",
    "\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "input_name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if input_name not in known_face_names:\n",
    "    print(f\"Error: {input_name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Set to keep track of names that have been recorded during the session\n",
    "recorded_names = set()\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)  # Mark attendance for recognized face\n",
    "                        recorded_names.add(name)\n",
    "                else:\n",
    "                    print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                    engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                    engine.runAndWait()\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836985eb-4493-48e3-b9a4-88ea9896f999",
   "metadata": {},
   "source": [
    "# Correct code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278560f3-eb36-4290-a1a7-b558dbd88170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  dfd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: dfd is not a recognized user.\n",
      "\n",
      "0: 480x640 1 face, 365.9ms\n",
      "Speed: 14.0ms preprocess, 365.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 185.1ms\n",
      "Speed: 4.0ms preprocess, 185.1ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 167.4ms\n",
      "Speed: 2.8ms preprocess, 167.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 150.2ms\n",
      "Speed: 1.4ms preprocess, 150.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 134.0ms\n",
      "Speed: 3.1ms preprocess, 134.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 156.0ms\n",
      "Speed: 2.2ms preprocess, 156.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 126.8ms\n",
      "Speed: 3.8ms preprocess, 126.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 147.3ms\n",
      "Speed: 3.9ms preprocess, 147.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 118.8ms\n",
      "Speed: 2.3ms preprocess, 118.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 122.6ms\n",
      "Speed: 1.6ms preprocess, 122.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 117.7ms\n",
      "Speed: 1.7ms preprocess, 117.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 108.9ms\n",
      "Speed: 0.0ms preprocess, 108.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# correct code \n",
    "# with voice for know unkown recognize and csv file all\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Path to the attendance CSV file\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set voice to Microsoft Zira Desktop - English (United States)\n",
    "voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "engine.setProperty('voice', voice_id)\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Function to append attendance to a CSV file and give voice feedback\n",
    "def mark_attendance(name):\n",
    "    global unknown_detected\n",
    "    \n",
    "    if not os.path.isfile(attendance_file):\n",
    "        # Create the file and write the header if it doesn't exist\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "    \n",
    "    if name == \"Unknown\":\n",
    "        if not unknown_detected:\n",
    "            print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.runAndWait()\n",
    "            unknown_detected = True  # Set flag to True after announcing once\n",
    "    else:\n",
    "        # Append the name with the current date and time\n",
    "        now = datetime.now()\n",
    "        date_str = now.strftime('%Y-%m-%d')\n",
    "        time_str = now.strftime('%H:%M:%S')\n",
    "        df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "        \n",
    "        # Voice feedback\n",
    "        engine.say(f\"{name}, attendance is marked\")\n",
    "        engine.runAndWait()\n",
    "\n",
    "# Initially load known faces\n",
    "load_known_faces()\n",
    "\n",
    "# Prompt user to enter their name\n",
    "input_name = input(\"Enter your name: \")\n",
    "\n",
    "# Match the entered name with known face labels\n",
    "if input_name not in known_face_names:\n",
    "    print(f\"Error: {input_name} is not a recognized user.\")\n",
    "    exit()\n",
    "\n",
    "# Set to keep track of names that have been recorded during the session\n",
    "recorded_names = set()\n",
    "\n",
    "# Initialize a variable to track unknown detection\n",
    "unknown_detected = False\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a file path for video file\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)  # Mark attendance for recognized face\n",
    "                        recorded_names.add(name)\n",
    "                else:\n",
    "                    if not unknown_detected:\n",
    "                        print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.runAndWait()\n",
    "                        unknown_detected = True\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cb4ef-d711-49b7-95ed-9af92480666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n-face.pt')  # Ensure you have a YOLOv8 model trained for face detection\n",
    "\n",
    "# Path to the known faces directory\n",
    "known_faces_dir = 'known_faces'\n",
    "\n",
    "# Path to the attendance CSV file\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set voice to Microsoft Zira Desktop - English (United States)\n",
    "voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "engine.setProperty('voice', voice_id)\n",
    "\n",
    "# Load known faces and their encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    # Extract the name without '_imgX' part\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "def mark_attendance(name):\n",
    "    global unknown_detected\n",
    "    \n",
    "    if not os.path.isfile(attendance_file):\n",
    "        # Create the file and write the header if it doesn't exist\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "    \n",
    "    if name == \"Unknown\":\n",
    "        if not unknown_detected:\n",
    "            print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.runAndWait()\n",
    "            unknown_detected = True  # Set flag to True after announcing once\n",
    "    else:\n",
    "        # Append the name with the current date and time\n",
    "        now = datetime.now()\n",
    "        date_str = now.strftime('%Y-%m-%d')\n",
    "        time_str = now.strftime('%H:%M:%S')\n",
    "        df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "        \n",
    "        # Voice feedback\n",
    "        engine.say(f\"{name}, attendance is marked\")\n",
    "        engine.runAndWait()\n",
    "\n",
    "def process_frame(frame, recorded_names, unknown_detected):\n",
    "    # Convert the frame to RGB (YOLO model expects RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(rgb_frame)\n",
    "    \n",
    "    # Convert results to a DataFrame-like structure\n",
    "    boxes = results[0].boxes.data.cpu().numpy()  # Get the boxes from the first result\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Extract the coordinates and confidence score\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        \n",
    "        # Append face location in the format required by face_recognition\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        \n",
    "        # Get face encodings for the detected face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            # Compare the face encoding with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)  # Mark attendance for recognized face\n",
    "                        recorded_names.add(name)\n",
    "                else:\n",
    "                    if not unknown_detected:\n",
    "                        print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.runAndWait()\n",
    "                        unknown_detected = True\n",
    "        \n",
    "        face_names.append(name)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the name\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    return frame, recorded_names, unknown_detected\n",
    "\n",
    "def main():\n",
    "    # Initially load known faces\n",
    "    load_known_faces()\n",
    "\n",
    "    # Prompt user to enter their name\n",
    "    input_name = input(\"Enter your name: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44b37116-a7f6-487c-be23-f78a40dd3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize YOLO model\n",
    "model = YOLO('yolov8n-face.pt')\n",
    "\n",
    "# Paths\n",
    "known_faces_dir = 'known_faces'\n",
    "attendance_file = 'attendance.csv'\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "engine.setProperty('voice', voice_id)\n",
    "\n",
    "# Load known faces\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7bebf00-535a-40ef-811f-1690703aecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "# Load known faces initially\n",
    "load_known_faces()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0b88d92-ae93-444c-92f5-7400b66fd325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_attendance(name):\n",
    "    global unknown_detected\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "\n",
    "    if name == \"Unknown\":\n",
    "        if not unknown_detected:\n",
    "            print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.runAndWait()\n",
    "            unknown_detected = True\n",
    "    else:\n",
    "        now = datetime.now()\n",
    "        date_str = now.strftime('%Y-%m-%d')\n",
    "        time_str = now.strftime('%H:%M:%S')\n",
    "        df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "\n",
    "        engine.say(f\"{name}, attendance is marked\")\n",
    "        engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b49f8567-90e9-4ef8-a15e-aeb8d2259798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  csdwsd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 320.1ms\n",
      "Speed: 4.7ms preprocess, 320.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 113.6ms\n",
      "Speed: 4.0ms preprocess, 113.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 127.5ms\n",
      "Speed: 3.9ms preprocess, 127.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 121.9ms\n",
      "Speed: 0.0ms preprocess, 121.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 119.7ms\n",
      "Speed: 3.1ms preprocess, 119.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "recorded_names = set()\n",
    "unknown_detected = False\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Take name input before starting the main loop\n",
    "input_name = input(\"Enter your name: \")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = model(rgb_frame)\n",
    "    boxes = results[0].boxes.data.cpu().numpy()\n",
    "\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "\n",
    "        name = \"Unknown\"\n",
    "        if face_encodings:\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)\n",
    "                        recorded_names.add(name)\n",
    "                else:\n",
    "                    if not unknown_detected:\n",
    "                        print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.runAndWait()\n",
    "                        unknown_detected = True\n",
    "\n",
    "        face_names.append(name)\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "251476e6-3c82-4fd8-83e3-329b3617f94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your ID:  dsds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 336.9ms\n",
      "Speed: 6.6ms preprocess, 336.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 185.3ms\n",
      "Speed: 5.4ms preprocess, 185.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 122.9ms\n",
      "Speed: 0.0ms preprocess, 122.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 140.9ms\n",
      "Speed: 3.1ms preprocess, 140.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 127.0ms\n",
      "Speed: 0.0ms preprocess, 127.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 112.1ms\n",
      "Speed: 0.0ms preprocess, 112.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 155.0ms\n",
      "Speed: 0.0ms preprocess, 155.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 137.7ms\n",
      "Speed: 0.0ms preprocess, 137.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 111.6ms\n",
      "Speed: 1.9ms preprocess, 111.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 111.4ms\n",
      "Speed: 1.5ms preprocess, 111.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize global variables\n",
    "known_faces_dir = 'known_faces'\n",
    "attendance_file = 'attendance.csv'\n",
    "engine = pyttsx3.init()\n",
    "voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "engine.setProperty('voice', voice_id)\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "unknown_detected = False  # Initialize unknown_detected\n",
    "recorded_names = set()\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "def mark_attendance(name):\n",
    "    global unknown_detected\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "\n",
    "    if name == \"Unknown\":\n",
    "        if not unknown_detected:\n",
    "            print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.runAndWait()\n",
    "            unknown_detected = True\n",
    "    else:\n",
    "        now = datetime.now()\n",
    "        date_str = now.strftime('%Y-%m-%d')\n",
    "        time_str = now.strftime('%H:%M:%S')\n",
    "        df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "\n",
    "        engine.say(f\"{name}, attendance is marked\")\n",
    "        engine.runAndWait()\n",
    "\n",
    "def initialize_yolo():\n",
    "    return YOLO('yolov8n-face.pt')\n",
    "\n",
    "def recognize_faces(frame, model):\n",
    "    global unknown_detected\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = model(rgb_frame)\n",
    "    boxes = results[0].boxes.data.cpu().numpy()\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)\n",
    "                        recorded_names.add(name)\n",
    "                else:\n",
    "                    if not unknown_detected:\n",
    "                        print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.runAndWait()\n",
    "                        unknown_detected = True\n",
    "        \n",
    "        face_names.append(name)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def start_recognition():\n",
    "    load_known_faces()\n",
    "    model = initialize_yolo()\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        exit()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "        \n",
    "        frame = recognize_faces(frame, model)\n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    input_name = input(\"Enter your ID: \")\n",
    "    # Optionally check if input_name is in known_face_names here\n",
    "    \n",
    "    start_recognition()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7f54486-76b6-4843-b2e7-2625453281bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize global variables\n",
    "known_faces_dir = 'known_faces'\n",
    "attendance_file = 'attendance.csv'\n",
    "engine = pyttsx3.init()\n",
    "voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "engine.setProperty('voice', voice_id)\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "unknown_detected = False  # Initialize unknown_detected\n",
    "recorded_names = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5a7d0e5-dd8d-4501-a90d-c913b94667ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_names\n",
    "    for root, _, files in os.walk(known_faces_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                img = face_recognition.load_image_file(filepath)\n",
    "                encodings = face_recognition.face_encodings(img)\n",
    "                if encodings:\n",
    "                    encoding = encodings[0]\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    known_face_names.append(name)\n",
    "\n",
    "def mark_attendance(name):\n",
    "    global unknown_detected\n",
    "    if not os.path.isfile(attendance_file):\n",
    "        df = pd.DataFrame(columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, index=False)\n",
    "\n",
    "    if name == \"Unknown\":\n",
    "        if not unknown_detected:\n",
    "            print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "            engine.runAndWait()\n",
    "            unknown_detected = True\n",
    "    else:\n",
    "        now = datetime.now()\n",
    "        date_str = now.strftime('%Y-%m-%d')\n",
    "        time_str = now.strftime('%H:%M:%S')\n",
    "        df = pd.DataFrame([[name, date_str, time_str]], columns=['Name', 'Date', 'Time'])\n",
    "        df.to_csv(attendance_file, mode='a', header=False, index=False)\n",
    "\n",
    "        engine.say(f\"{name}, attendance is marked\")\n",
    "        engine.runAndWait()\n",
    "\n",
    "def initialize_yolo():\n",
    "    return YOLO('yolov8n-face.pt')\n",
    "\n",
    "def recognize_faces(frame, model):\n",
    "    global unknown_detected\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = model(rgb_frame)\n",
    "    boxes = results[0].boxes.data.cpu().numpy()\n",
    "    face_locations = []\n",
    "    face_names = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2, confidence = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4]\n",
    "        face_locations.append((y1, x2, y2, x1))\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [face_locations[-1]])\n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if face_encodings:\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                    if name not in recorded_names:\n",
    "                        mark_attendance(name)\n",
    "                        recorded_names.add(name)\n",
    "                else:\n",
    "                    if not unknown_detected:\n",
    "                        print(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.say(\"Unknown person detected. Attendance cannot be marked.\")\n",
    "                        engine.runAndWait()\n",
    "                        unknown_detected = True\n",
    "        \n",
    "        face_names.append(name)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b4709df-9112-4b31-8f95-e3f20bd6bc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  fdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 face, 127.9ms\n",
      "Speed: 9.7ms preprocess, 127.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Unknown person detected. Attendance cannot be marked.\n",
      "\n",
      "0: 480x640 1 face, 163.1ms\n",
      "Speed: 3.3ms preprocess, 163.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 139.0ms\n",
      "Speed: 0.0ms preprocess, 139.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 138.6ms\n",
      "Speed: 3.0ms preprocess, 138.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 122.3ms\n",
      "Speed: 2.8ms preprocess, 122.3ms inference, 13.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "def start_recognition():\n",
    "    load_known_faces()\n",
    "    model = initialize_yolo()\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "     # Prompt user to enter their name\n",
    "    input_name = input(\"Enter your name: \")\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        exit()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "        \n",
    "        frame = recognize_faces(frame, model)\n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "start_recognition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecded54-5eab-431d-b3a2-e1b1fe55fb21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
